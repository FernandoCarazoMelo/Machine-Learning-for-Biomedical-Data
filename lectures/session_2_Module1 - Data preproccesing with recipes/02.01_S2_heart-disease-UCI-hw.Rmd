---
title: MODULE 1. Predicting Heart Attack. A Complete Machine Lerning Project Using R and Caret 
date: 2021-02-04
output: 
  html_document:
    theme: spacelab
    highlight: haddock
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
    number_sections: true
    
author: Fernando Carazo^1^ 
# bibliography: "references.bib" # Create file and uncomment. Cite as ej [@Ignatiadis_2016]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# library("knitcitations")
#cleanbib()
options("citation_format" = "pandoc")
# write.bibtex(file="references.bib")
```

Contact info: **Fernando Carazo** <fcarazo@tecnun.es>

[*^1^Department of Biomedical Engineering and Sciences, Tecnun. University of Navarre*](https://www.unav.edu/en/web/departamento-de-ingenieria-biomedica-y-ciencias/investigacion/computational-biology/presentacion)


<p align="center">
    <img src="https://raw.githubusercontent.com/carlosdg/PrediccionEnfermedadCoronaria/master/docs/images/HeartImage__GordonJohnson__Pixabay.png" alt="Image of a heart and an electrocardiogram with two normal heart beat" />
</p>




http://www.dalcame.com/images/ecg.jpg

# Introduction

According to the National Heart, Lung and Blood Institute:

> Heart disease is a catch-all phrase for a variety of conditions that affect the heart‚Äôs structure and function. Coronary heart disease is a type of heart disease that develops when the arteries of the heart cannot deliver enough oxygen-rich blood to the heart. __It is the leading cause of death in the United States__.

(Emphasis by me. Source: https://www.nhlbi.nih.gov/health-topics/espanol/enfermedad-coronaria)

Also, according to the World Health Organization, cardiovascular diseases are the __leading cause of death globally__ (source:  https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)). 

In this notebook we try to learn enough information of this topic to understand the [Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci) dataset and build simple models to predict whether a patient has a disease or not based on features like the heart rate during exercise or the cholesterol levels in the blood.



# Context

## Blood and heart

Blood is very important to ensure the proper functioning of the body. Its functions cover the transport of oxygen and nutrients to the cells of the body as well as the removal of the cellular waste products.

Blood is transported to the rest of the body because it is pumped by the heart. This organ receives oxygen-poor blood and sends it to the lungs to oxygenate it. And sends the oxygen-rich blood that comes from the lugns to the rest of the body.

<p align="center">
  <img src="https://raw.githubusercontent.com/carlosdg/PrediccionEnfermedadCoronaria/master/docs/images/Latidos.gif" alt="Blood flow through the chambers of the heart" />
</p>

>By josi√±o - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=9396374. Flow of the blood through the chambers of the heart. Blue arrows represent oxygen-poor blood received from the rest of the body and sent to the lungs. Red arrows represent oxygen-rich blood coming from the lungs that is sent to the rest of the body.

An inadequate supply of the blood can yield the cells to not get enough energy to function properly, causing the death of the cells in the worst case.


## Coronary heart disease

The heart also needs oxygen and nutrients to function properly, these come through arteries known as coronary arteries. When we talk about a coronary disease, we often mean a difficulty of the blood flow in these arteries due to the accumulation of substances on their walls.

<p align="center" class="figure">
  <img src="https://raw.githubusercontent.com/carlosdg/PrediccionEnfermedadCoronaria/master/docs/images/Heart_attack-NIH.gif" alt="Death of heart cells due to an ischemia in the coronary arteries">
</p>

>By NIH: National Heart, Lung and Blood Institute - http://www.nhlbi.nih.gov/health/health-topics/topics/heartattack/, Public Domain, https://commons.wikimedia.org/w/index.php?curid=25287085. Death of heart cells due to an ischemia in the coronary arteries.

In the worst case, the impact of leaving the cells of the heart without nutrients and oxygen is a heart attack, in other words, the death of part of the heart cells. This, in turn, would have an impact on the rest of the body because the pumping of the heart would be affected.


## Glossary of terms

- **Atherosclerosis:** accumulation of substances on the walls of arteries which can hinder the blood flow. Moreover, the rupture of this plaque of substances can cause the formation of a blood clot (thrombus) that, in turn, can block even more the affected area or go to other parts of the body and block those parts (embolism). (Sources: [American Heart Association](https://www.heart.org/en/health-topics/cholesterol/about-cholesterol/atherosclerosis), [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/arteriosclerosis-atherosclerosis/symptoms-causes/syc-20350569))

- **Ischemia:** blood flow reduction to a tissue. This implies a reduction of the supply of oxygen and nutrients, so cells won't get enough energy to function properly. (Sources: [American Heart Association](https://www.heart.org/en/health-topics/heart-attack/about-heart-attacks/silent-ischemia-and-ischemic-heart-disease), [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/myocardial-ischemia/symptoms-causes/syc-20375417), [Wikipedia](https://en.wikipedia.org/wiki/Ischemia))

- **Angina:** chest pain due to a blood flow reduction in the coronary arteries. (Sources: [United Kingdom National Health Service](https://www.nhs.uk/conditions/angina/), [(Spanish) Video sobre angina de Alberto Sanagust√≠n](https://www.youtube.com/watch?v=3UhItS50mRI))

- **Stable angina:** angina caused by situations that requires oxygen (for example, exercise or stress) and it goes away on rest.

- **Unstable angina:** angina that can happen even on rest.

- **Typican & atypical angina:** typical angina usually means a chest disconfort. However, looks like some people can experience other symptoms like nausea or shortness of breath. In these cases people talk about atypical angina. (Sources: [Harrington Hospital](https://www.harringtonhospital.org/typical-and-atypical-angina-what-to-look-for/), [Wikipedia](https://en.wikipedia.org/wiki/Angina#Diagnosis)).

- **Thrombus:** blood mass in solid state that hinders the blood flow in a blood vessel. (Source: [MedlinePlus](https://medlineplus.gov/ency/article/001124.htm))

- **Embolus:** thrombus that detatches and goes to other parts of the body. (Source: [MedlinePlus](https://medlineplus.gov/ency/article/001124.htm))

- **Acute myocardial infarction:** also known as _heart attack_, is the death of part of the heart tissue due to an ischemia. In other words, it is the death of part of the cells due to the lack of oxygen. (Sources: [Healthline](https://www.healthline.com/health/acute-myocardial-infarction#causes), [Wikipedia](https://en.wikipedia.org/wiki/Infarction))

- **Electrocardiogram:** graph record of the electric signals that causes heart beats. Each part of the record of a normal heart beat has a name, the most interesting ones for this project are the T wave and the ST segment because they can give some information about the presence of issues like an ischemia or infarction. (Sources: [Mayo Clinic](https://www.mayoclinic.org/tests-procedures/ekg/about/pac-20384983), [Wikipedia](https://es.wikipedia.org/wiki/Electrocardiograma), [(Spanish) Video sobre electrocardiograma de Alberto Sanagust√≠n](https://www.youtube.com/watch?v=A67NDj3_RrU), [(Spanish) Serie de videos sobre el electrocardiograma normal de Alberto Sangaust√≠n](https://www.youtube.com/watch?v=SLe281LBBRU&list=PL3BE5DA0A5DF3BF75))

- **Nuclear stress test:** a radioactive dye is injected into the patient to see the blood flow on rest and doing exercise. Moreover, during this test the activity of the heart is also measured with an electrocardiogram. (Sources: [Mayo Clinic](https://www.mayoclinic.org/tests-procedures/nuclear-stress-test/about/pac-20385231), [Healthline](https://www.healthline.com/health/thallium-stress-test))

- **Asymptomatic disease:** a disease that a patient has but they experience very few or no symptoms. (Sources: [(Spanish) definicion.de](https://definicion.de/asintomatico/), [MayoClinic](https://www.mayoclinic.org/es-es/diseases-conditions/heart-attack/expert-answers/silent-heart-attack/faq-20057777), [Wikipedia](https://en.wikipedia.org/wiki/Asymptomatic))

- **Left ventricular hypertrophy:** thickening of the walls of the main heart chamber that pumps the blood to the rest of the body. This can cause the muscle to loose elasticity which, in turns, causes the heart to not work properly. (Sources: [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/left-ventricular-hypertrophy/symptoms-causes/syc-20374314))


# Dataset

This dataset is hosted on Kaggle ([Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci)), and it was from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease). There are records of about 300 patients from Cleveland and the features are described in a following section.

According to the information provided by Kaggle, the available variables are:

Context

This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. 

*The "goal" field refers to the presence of heart disease in the patient.*

Content

Attribute Information:

1. age
2. sex
3. chest pain type (4 values)
4. resting blood pressure
5. serum cholestoral in mg/dl
6. fbs: fasting blood sugar > 120 mg/dl

Hereon, variables are related to a nuclear stress test. That is, a stress test where a radioactive dye is also injected to the patient to see the blood flow:

7. restecg: resting electrocardiographic results (values 0,1,2)
8. thalach: maximum heart rate achieved
9. exang: exercise induced angina
10.oldpeak: ST depression induced by exercise relative to rest
11. slope: the slope of the peak exercise ST segment
12. number of major vessels (0-3) colored by flourosopy
13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect

The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been "processed", that one containing the Cleveland database. All four unprocessed files also exist in this directory.

<!-- ## Dataset problems -->

<!-- Thanks to the [post of _InitPic_ ](https://www.kaggle.com/ronitf/heart-disease-uci/discussion/105877) we noted that this dataset is a bit different from the original one while the description is the same.  -->

<!-- Part of these differences is that there were a few null values in the original dataset that have taken some values here: -->

<!-- > A few more things to consider: -->
<!-- > -->
<!-- > data #93, 159, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed) -->
<!-- > -->
<!-- > data #49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset. -->

<!-- Because these are just a few instances, I decided to drop them. -->

<!-- There are also some differences regarding the features of the dataset which are corrected below. -->

```{r}
# Libraries
library(ggplot2)
library(caret)
library(rpart.plot)
library(tidyverse)
library(skimr)
library(ggpubr)

# Read the data
data <- read.csv('./data/heart_mod_2021-02-04.csv')

dim(data)
colnames(data)


```


# Exploratory data analysis

Before training a predictive model, or even before performing any calculation with a new data set, it is important to carry out a descriptive exploration of them. This process allows a better understanding of what information each variable contains, as well as detecting possible errors. Some common examples are:

- That a column has been stored with the wrong type: a numeric variable is being recognized as text or vice versa.

- That a variable contains values that do not make sense: for example, to indicate that the price of a home is not available, enter the value 0 or a blank space.

- That a word has been entered in a numeric type variable instead of a number.

Furthermore, this initial analysis can give clues as to which variables are suitable as predictors in a model (more on this in the following sections).

The skimr, DataExplorer and GGally packages make this task a lot easier thanks to their pre-configured functions.

## Variable type

Before training a predictive model, or even before performing any calculation with a new data set, it is very important to carry out a descriptive exploration of them. This process allows us to better understand what information each variable contains, as well as detect possible errors. Some common examples are:

- That a column has been stored with the wrong type: a numeric variable is being recognized as text.

- That a variable contains values that do not make sense: e.g, that the height of a person is not available, enter the value zero or a blank space. There is no one whose height is zero.

- That a word has been entered in a numeric type variable instead of a number.

- Furthermore, it can give clues as to which variables are not suitable as predictors in a model (more on this in the following sections).


```{r}
glimpse(data)
summary(data)
skim(data)

```



## Dataset features


<!-- ```{r} -->
<!-- # this chunk should be removed after crating new data -->
<!-- data <- data_new -->

<!-- data_new$ca[which(data$ca %in% 4)] <- NA -->
<!-- data_new$thal[which(data$thal %in% 0)] <- NA -->

<!-- write.csv(data, "./data/heart_mod_2021-02-04.csv") -->


<!-- ``` -->



<!-- ```{r} -->
<!-- # Drop the null values -->
<!-- missing_ca_indeces <- which(data$ca %in% 4) -->
<!-- missing_thal_indeces <-which(data$thal %in% 0) -->
<!-- missing_values_indeces <- c(missing_ca_indeces, missing_thal_indeces) -->
<!-- data <- data[-missing_values_indeces, ] -->
<!-- ``` -->


```{r}

# remove X column
data_rd <- data
try(data <- data %>% select(-X))

# Transform categorical variable to R factors
data$sex <- as.factor(data$sex)
data$cp <- as.factor(data$cp) # Chest pain type (4 types)
data$fbs <- as.factor(data$fbs) # Whether the level of sugar in the blood is higher than 120 mg/dl or not. 
data$restecg <- as.factor(data$restecg) # Results of the electrocardiogram on rest (3 types)
data$exang <- as.factor(data$exang) # Whether the patient had angina during exercise
data$slope <- as.factor(data$slope) # Slope of the ST segment during the most demanding part of the exercise (3 types)
data$thal <- as.factor(data$thal) # Results of the blood flow observed via the radioactive dye. (3 types)
data$target <- as.factor(data$target) # heart attack y/n

# Give a better name to the factor values for the graphs
levels(data$sex) <- c("Female", "Male")
levels(data$cp) <- c("Asymptomatic", "Atypical angina", "No angina", "Typical angina")
levels(data$fbs) <- c("No", "Yes")
levels(data$restecg) <- c("Hypertrophy", "Normal", "Abnormalities")
levels(data$exang) <- c("No", "Yes")
levels(data$slope) <- c("Descending", "Flat", "Ascending")
levels(data$thal) <- c("Fixed defect", "Normal flow", "Reversible defect")
levels(data$target) <- c("Yes", "No")
```

> YOR TURN: INPECT ALL VARIABLES AND MAKE YOUR HYPOTHESES OF HOW EACH VARIABLE AFFECTS HEART ATTACK INCIDENCE ("target" column) 

### target

Target variable: whether the patient has a heart disease or not

- Value 0: yes
- Value 1: no

We can see that the distribution is quite balanced. Thanks to this it wouldn't be a bad idea using accuracy to evaluate how well the models perform.

```{r}
ggplot(data, aes(target, fill=target)) + 
  geom_bar() +
  labs(x="Disease", y="Number of patients") +
  guides(fill=FALSE)

```

### age

Patient age in years. In the data we can see, as expected, that age is a risk factor. In other words, the higher the age, the more likely that the patient has a heart disease.

```{r}
ggplot(data, aes(age, fill=target)) + 
  geom_histogram(binwidth=1) +
  labs(fill="Disease", x="Age", y="Number of patients")


data %>% ggplot(aes(target, age)) +
  geom_boxplot(aes(fill= target))


```

### sex

Patient sex

- Value 0: female
- Value 1: male

There are approximately half the observation of women than men. We can also see that sex is a risk factor, like some of the references indicate, men are more likely to have a heart disease than women.

```{r}
ggplot(data, aes(sex, fill=target)) + 
  geom_bar() +
  labs(fill="Disease", x="Sex", y="Number of patients")


ggplot(data, aes(sex, fill=target)) + 
  geom_bar(position = "fill") +
  labs(fill="Disease", x="Sex", y="Number of patients", title = "Patients")

```

### cp

Chest pain type

- Value 0: asymptomatic
- Value 1: atypical angina
- Value 2: pain without relation to angina
- Value 3: typical angina

The description of the data doesn't provide information about how this classification of pain was made. But we can see that it is very difficult to tell whether a patient has a heart disease attending just to the symptoms of the patients.

```{r}
ggplot(data, aes(cp, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Chest pain type", y="Number of patients")


ggplot(data, aes(cp, fill=target)) +
  geom_bar(position = "fill") +
  labs(fill="Disease", x="Chest pain type", y="Number of patients")

```

### trestbps

Resting blood pressure in millimeters of mercury (mm Hg) when the patient was admitted to the hospital.

By the different peaks, looks like most people tend to have a normal blood pressure inside certain groups (could be healthy adults, adults that take medication, seniors...). It also looks like very high pressures can indicate that there is a heart disease.

```{r}
ggplot(data, aes(trestbps, fill=target)) +
  geom_histogram(binwidth=3) +
  labs(fill="Disease", x="Blood pressure (mm Hg)", y="Number of patients")

# sort(table(data$trestbps), decreasing = T)
```


### chol

Cholesterol level in mg/dl. This is a variable that we can control to prevent the disease. Looks like the majority of the people in the dataset have high levels of cholesterol. It also looks like up to a certain level, the presence of a heart disease is slightly higher on higher cholesterol levels. Though the cases that have the highest levels of cholesterol don't have a heart disease, it could be that these people weren't fasting when the blood sample was taken.

```{r}
ggplot(data, aes(chol, fill=target)) +
  geom_histogram(binwidth=10) +
  labs(fill="Disease", x="Cholesterol (mg/dl)", y="Number of patients")

data %>% ggplot(aes(target, chol)) +
  geom_boxplot(aes(fill = sex))

data %>% ggplot(aes(sex, chol)) +
  geom_boxplot(aes(fill = target))
```


### fbs

Whether the level of sugar in the blood is higher than 120 mg/dl or not. This is another variable that we can control. However, by itself it doesn't seem very useful to know if a patient has a heart disease or not. Though we shouldn't drop it right now because it might be useful combined with other variables.

- Value 0: no
- Value 1: yes

```{r}
ggplot(data, aes(fbs, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Sugar level > 120 mg/dl", y="Number of patients")

ggplot(data, aes(fbs, fill=target)) +
  geom_bar(position = "fill") +
  labs(fill="Disease", x="Sugar level > 120 mg/dl", y="Number of patients")
```


*Hereon, variables are related to a nuclear stress test. That is, a stress test where a radioactive dye is also injected to the patient to see the blood flow.*

> [ECG Expanation](https://www.youtube.com/watch?v=Xl4Hmm_dwew&ab)



<p align="center">
    <img src="http://www.dalcame.com/images/ecg.jpg" alt="Image of an electrocardiogram with two normal heart beat" />
</p>

### restecg

Results of the electrocardiogram on rest

- Value 0: probable left ventricular hypertrophy
- Value 1: normal
- Value 2: abnormalities in the T wave or ST segment

When someone has a heart disease the first symptom usually is stable angina (angina during exercise). When angina happens even on rest the disease got worse (usually due to a narrowing of the coronary arteries). This has to be why there are so few patients that show an abnormality on the heart rate on rest, and it is also why seeing this abnormality is very indicative of a presence of a heart disease. 

On the other hand, the value 0, *probable* presence of an hipertrophy, doesn't seem to be very indicative of the presence of a heart disease by itself. It can be because this variable is not very accurate (as noted by the "probable presence").

```{r}
ggplot(data, aes(restecg, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Electrocardiogram on rest", y="Number of patients")
```


### thalach

Maxium heart rate during the stress test

At first sight it may seem weird to see that the higher the heart rate the lower the presence of a heart disease and vice versa. However, it makes sense taking into account that the maximum healthy heart rate depends on the age (220 - age). Thus, higher rates tend to be from younger people.

```{r}
ggplot(data, aes(thalach, fill=target)) +
  geom_histogram(binwidth=10) +
  labs(fill="Disease", x="Maximum heart rate during exercise", y="Number of patients")
```

**Patients with a heart rate during exercise lower than 100:**

```{r}
data[data$thalach < 100, c("age", "thalach", "target")]
```

**Patients with a heart rate during exercise higher than 180:**

```{r}
data[data$thalach > 180, c("age", "thalach", "target")]
```


### exang

Whether the patient had angina during exercise

- Valor 0: no
- Valor 1: yes

We can see that this feature is a good indicator for the presence of heart disease. However, we can also see that knowing what is angina and what not is not an easy task, it can be confused with other pains or it can be atypical angina.

```{r}
ggplot(data, aes(exang, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Presence of angina during exercise", y="Number of patients")

ggplot(data, aes(exang, fill=target)) +
  geom_bar(position = "fill" ) +
  labs(fill="Disease", x="Presence of angina during exercise", y="Number of patients")
```


### oldpeak

Decrease of the ST segment during exercise according to the same one on rest.

The ST segment is a part of the electrocardiogram of a heart beat that is usually found at a certain level in a normal heart beat. A significant displacement of this segment can indicate the presence of a heart disease as we can see in the plot.

```{r}
ggplot(data, aes(oldpeak, fill=target)) +
  geom_histogram(binwidth=0.25) +
  labs(fill="Disease", x="Depression of the ST segment", y="Number of patients")
```


### slope

Slope of the ST segment during the most demanding part of the exercise

- Value 0: descending
- Value 1: flat
- Value 2: ascending

<p align="center">
    <img src="https://litfl.com/wp-content/uploads/2018/10/ST-segment-depression-upsloping-downsloping-horizontal.png" alt="Image of ST slope" />
</p>



In the first graph we can see that the slope by itself can help determine whether there is a heart disease or not if it is flat or ascending. However, if the slope is descending doesn't seem to give much information. Because of this, in the second graph we add a third variable where we can see that, if the slope is descending, the depression of the ST segment can help to determine if the patient has a heart disease.

```{r}
ggplot(data, aes(slope, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Slope of the ST segment", y="Number of patients")
```

```{r}
ggplot(data, aes(x=slope, y=oldpeak, fill=target)) +
  geom_boxplot() +
  labs(fill="Disease", x="Slope of the ST segment", y="Depression of the ST segment")
```


### thal

Results of the blood flow observed via the radioactive dye.

- Value 0: NULL (dropped from the dataset previously)
- Value 1: fixed defect (no blood flow in some part of the heart)
- Value 2: normal blood flow
- Value 3: reversible defect (a blood flow is observed but it is not normal)

This feature and the next one are obtained through a very invasive process for the patients. But, by themselves, they give a very good indication of the presence of a heart disease or not.

```{r}
ggplot(data, aes(thal, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Results of the blood flow", y="Number of patients")
```


### ca

<!-- Number of main blood vessels coloured by the radioactive dye. The number varies between 0 to 4 but the value 4 represents a null value and these have been dropped previously. -->

This feature refers to the number of narrow blood vessels seen, this is why the higher the value of this feature, the more likely it is to have a heart disease.

```{r}
ggplot(data, aes(ca, fill=target)) +
  geom_bar() +
  labs(fill="Disease", x="Number of main blood vessels coloured", y="Number of patients")
```


# Division of data in training and testing Data preprocessing




```{r}
# Helper packages
library(dplyr)    # for data manipulation
library(ggplot2)  # for awesome graphics
library(visdat)   # for additional visualizations

# Feature engineering packages
library(caret)    # for various ML tasks
library(recipes)  # for feature engineering tasks
```



A major goal of the machine learning process is to find an algorithm  $f(X)$  that most accurately predicts future values $\hat{Y}$ based on a set of features $X$. In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we ‚Äúspend‚Äù our data will help us understand how well our algorithm generalizes to unseen data.

To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets:

- Training set: these data are used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production).

- Test set: having chosen a final model, these data are used to estimate an unbiased assessment of the model‚Äôs performance, which we refer to as the generalization error.

> It is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process.

Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60% (training)‚Äì40% (testing), 70%‚Äì30%, or 80%‚Äì20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind:

- Spending too much in training (e.g., >80%) won‚Äôt allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting).

- Sometimes too much spent in testing (>40%) won‚Äôt allow us to get a good assessment of model parameters.
Other factors should also influence the allocation proportions. For example, very large training sets (e.g.,n>100K) often result in only marginal gains compared to smaller sample sizes. Consequently, you may use a smaller training sample to increase computation speed (e.g., models built on larger training sets often take longer to score new data sets in production). In contrast, as ($p‚â•n$), where $p$ represents the number of features, larger samples sizes are often required to identify consistent signals in the features.

The two most common ways of splitting data include simple random sampling and stratified sampling.

## Random sampling

```{r}
library(rsample)
set.seed(123)

split_1  <- initial_split(data, prop = 0.8)
train_1  <- training(split_1)
test_1  <- testing(split_1)
```

## Stratified sampling

If we want to explicitly control the sampling so that our training and test sets have similar  $Y$ distributions, we can use stratified sampling. This is more common with classification problems where the response variable may be severely imbalanced (e.g., 90% of observations with response ‚ÄúYes‚Äù and 10% with response ‚ÄúNo‚Äù). However, we can also apply stratified sampling to regression problems for data sets that have a small sample size and where the response variable deviates strongly from normality (i.e., positively skewed like Sale_Price). With a continuous response variable, stratified sampling will segment $Y$ into quantiles and randomly sample from each. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets.

The easiest way to perform stratified sampling on a response variable is to use the `sample` package, where you specify the response variable to stratafy. The following illustrates that in our original employee attrition data we have an imbalanced response. By enforcing stratified sampling, both our training and testing sets have approximately equal response distributions.


```{r}


```




# Feature and Target Engineerig

Data preprocessing and engineering techniques generally refer to the addition, deletion, or transformation of data. The time spent on identifying data engineering needs can be significant and requires you to spend substantial time understanding your data‚Ä¶or as Leo Breiman said ‚Äúlive with your data before you plunge into modeling‚Äù (Breiman and others 2001, 201). Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm‚Äôs predictive ability and deserves your continued focus and education.

We will not cover all the potential ways of implementing feature engineering; however, we‚Äôll cover several fundamental preprocessing tasks that can potentially significantly improve modeling performance. Moreover, different models have different sensitivities to the type of target and feature values in the model and we will try to highlight some of these concerns. For more in depth coverage of feature engineering, please refer to Kuhn and Johnson (2019) and Zheng and Casari (2018).

## Target engineering

Although not always a requirement, transforming the response variable can lead to predictive improvement, especially with parametric models (which require that certain assumptions about the model be met). For instance, ordinary linear regression models assume that the prediction errors (and hence the response) are normally distributed. This is usually fine, except when the prediction target has heavy tails (i.e., outliers) or is skewed in one direction or the other. In these cases, the normality assumption likely does not hold. 


**Option 1**: normalize with a log transformation. This will transform most right skewed distributions to be approximately normal. One way to do this is to simply log transform the training and test set in a manual, single step manner similar to:

<p align="center">
    <img src="https://bradleyboehmke.github.io/HOML/03-engineering_files/figure-html/engineering-skewed-residuals-1.png" alt="Example feature engineering" />
</p>

If your response has negative values or zeros then a log transformation will produce NaNs and -Infs, respectively (you cannot take the logarithm of a negative number). If the nonpositive response values are small (say between -0.99 and 0) then you can apply a small offset such as in log1p() which adds 1 to the value prior to applying a log transformation (you can do the same within step_log() by using the offset argument). If your data consists of values  ‚â§‚àí1, use the Yeo-Johnson transformation mentioned next.

**Option 2**: use a Box Cox transformation. A Box Cox transformation is more flexible than (but also includes as a special case) the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution (Box and Cox 1964; Carroll and Ruppert 1981). At the core of the Box Cox transformation is an exponent, $\lambda$  are considered and the optimal value for the given data is estimated from the training data; The ‚Äúoptimal value‚Äù is the one which results in the best transformation to an approximate normal distribution. 

The transformation of the response has the form:


<p align="center">
    <img src="https://miro.medium.com/max/884/1*_nHvGg-OQYtNSSZph-424g.png" alt="Example feature engineering boxcox" />
</p>




<p align="center">
    <img src="https://lh3.googleusercontent.com/proxy/O95HtsfIgGJZOcOoPlz376vkB8IpBzUdAnZLmVkxygfDtVkYs4vZ0dW0pX6c0MFEdeiR6xjGiOIznUANR3mUregzHDudVT2YSE-xsoWzw8hY83ZhrlImmefFsgsR" alt="Example feature engineering boxcox" />
</p>




> Be sure to compute the `lambda` on the training set and apply that same `lambda` to both the training and test set to minimize data leakage. The recipes package automates this process for you.
<br>
If your response has negative values, the Yeo-Johnson transformation is very similar to the Box-Cox but does not require the input variables to be strictly positive. To apply, use step_YeoJohnson().


```{r}



```



## Features engineering

Data preprocessing encompasses those transformations of the data made in order that they can be accepted by the machine learning algorithm or that improve their results. All preprocessing of data must be learned from training observations and then applied to the training set and the test set. This is very important so as not to violate the condition that no information from test observations can participate in or influence the fit of the model. Although it is not possible to create a single listing, some preprocessing steps that are most often applied in practice are:

- Imputation of missing values

- Exclusion of variables with variance close to zero

- Dimensionality reduction

- Standardization of numerical variables

- Binarization of qualitative variables

The  caret` package incorporates many functions to pre-process the data. However, to make learning transformations even easier with training observations only, and then apply them to any dataset, the same author has created the r recipes` package.

The idea behind this package is as follows:

1. Define which is the response variable, the predictors and the training data set, ` recipe ()`.

2. Define all the transformations (scaling, selection, filtering ...) that you want to apply, ` %>% step _ ()`.

3. Learn the necessary parameters for these transformations with the training observations ` prep ()`.

4. Apply the learned transformations to any ` bake ()` dataset.

In the following sections, all the preprocessing steps are stored in a recipe object and finally applied to the data.


## Imputation of missing values

As identified in the data several variables contain missing values. The vast majority of algorithms do not accept incomplete observations, so when the data set contains missing values, you can:

- Delete those observations that are incomplete.

- Delete those variables that contain missing values.

- Try to estimate the missing values using the rest of the available information (imputation).

The first two options, although simple, involve losing information. Deletion of observations can only be applied when many are available and the percentage of incomplete records is very low. In the case of eliminating variables, the impact will depend on how much information these variables contribute to the model.

When imputation is used, it is very important to take into account the risk involved when entering values in predictors that have a lot of influence on the model. Suppose a medical study in which, when one of the predictors is positive, the model almost always predicts that the patient is healthy. For a patient whose value of this predictor is unknown, the risk that the imputation is wrong is very high, so it is preferable to obtain a prediction based only on the available information. This is another example of the importance of the analyst knowing the problem he is facing and thus making the best decision.

In this data set, we only have 7 missing values so they could be removed. However, we decide to impute those values.

```{r}
sum(is.na(data))
```


```{r}


```

If a variable is absent for ~80% of the observations, with such a high percentage of absent values, it is not convenient to impute it, it is excluded directly from the model. 

Imputation is a complex process that must be carried out carefully, carefully identifying which variables are appropriate for each imputation. For more details on why they are carried out as follows, see Annex 7.

The recipes package allows 4 different imputation methods:

- ` step_meanimpute ()`: mean imputation via the predictor (continuous predictors).

- ` step_modeimpute ()`: imputation via predictor mode (qualitative predictors).

- ` step_bagimpute ()`: imputation via Bagged Trees.

- ` step_knnimpute ()`: imputation via K-Nearest Neighbors.



Also noteworthy are the Hmisc missForest and MICE packages that allow other methods to be applied.


```{r}



```


Figure illustrates the differences between mean, KNN, and tree-based imputation on the raw Ames housing data. It is apparent how descriptive statistic methods (e.g., using the mean and median) are inferior to the KNN and tree-based imputation methods.


<p align="center">
    <img src="https://bradleyboehmke.github.io/HOML/03-engineering_files/figure-html/engineering-imputation-examples-1.png" alt="Image of a heart and an electrocardiogram with two normal heart beat" />
</p>

> Figure: Comparison of three different imputation methods. The red points represent actual values which were removed and made missing and the blue points represent the imputed values. Estimated statistic imputation methods (i.e. mean, median) merely predict the same value for each observation and can reduce the signal between a feature and the response; whereas KNN and tree-based procedures tend to maintain the feature distribution and relationship.


## Variables with variance close to zero

Predictors that contain a single value (zero-variance) should not be included in the model since they do not provide information. It is also not convenient to include predictors that have a variance close to zero, that is, predictors that take only a few values, some of which appear very infrequently. The problem with the latter is that they can become predictors with zero variance when dividing observations by cross-validation or bootstrapping.

The nearZeroVar () function in the caret package and step_nzv () in the recipe package identify potentially problematic predictors as those that have a single value (zero variance) or that meet the following two conditions:

- Frequency ratio: ratio between the frequency of the most common value and the frequency of the second most common value. This ratio tends to 1 if the frequencies are equally distributed and to large values when the frequency of the majority value is much higher than the rest (the denominator is a small decimal number). Default value freqCut = 95/5.

- Percentage of unique values: number of unique values divided by the total number of samples (multiplied by 100). This percentage approaches zero the greater the variety of values. Default value uniqueCut = 10.


```{r}


```


```{r}

```

## Standardization and scaling

When the predictors are numerical, the scale on which they are measured, as well as the magnitude of their variance, can greatly influence the model. Many machine learning algorithms (SVM, neural networks, lasso ...) are sensitive to this, so that if the predictors are not somehow equalized, those that are measured on a larger scale or have more variance will dominate the model. although they are not the ones that have the most relationship with the response variable. There are mainly 2 strategies to avoid it:

* Centered: it consists of subtracting from each value the mean of the predictor to which it belongs. If the data is stored in a dataframe, the centering is achieved by subtracting from each value the mean of the column in which it is located. As a result of this transformation, all predictors have a mean of zero, that is, the values are centered around the origin.

* Normalization (standardization): consists of transforming the data so that all the predictors are approximately on the same scale. There are two ways to achieve this:

  + Z-score normalization: divide each predictor by its standard deviation after it has been centered, in this way, the data has a normal distribution.
  $ z = (x- \ mu) / \ sigma $

  + Max-min standardization: transform the data so that they are within the range [0, 1].
$ (X-Xmin) / (Xmax-Xmin) $

For this analysis, all numerical variables are normalized.

```{r}

```

## Binarization of qualitative variables

Binarization consists of creating new dummy variables with each of the levels of the qualitative variables. This process is also known as one hot encoding. For example, a variable called color containing the red, green, and blue levels will become three new variables (color_red, color_green, color_blue), all with the value 0 except the one that matches the observation, which takes the value 1.

By default, the step_dummy (all_nominal ()) function binarizes all stored variables as factor or character type. In addition, it eliminates one of the levels to avoid redundancies. Going back to the previous example, it is not necessary to store all three variables, since if color_red and color_green take the value 0, the variable color_blue necessarily takes the value 1. If color_red or color_green take the value 1, then color_blue is necessarily 0.

```{r}


```
Once the recipe object has been created with all the preprocessing transformations, they are learned with the training data and applied to the two sets.


```{r}

```


```{r}

```



# Creation of a predictive model


Once the data has been preprocessed and the predictors selected, the next step is to use a machine learning algorithm that allows creating a model capable of representing the patterns present in the training data and generalizing them to new observations. Finding the best model is not easy, there are many algorithms, each with its own characteristics and with different parameters that must be adjusted. In general, the steps followed to obtain a good model are:

Adjustment / training: consists of applying a machine learning algorithm to the training data so that the model learns.

Evaluation / Validation: The goal of a predictive model is not to be able to predict observations that are already known, but new observations that the model has not seen. In order to estimate the error that a model makes, it is necessary to resort to validation strategies, among which stand out test set, bootstrap and cross validation. Annex 4

Optimization of hyperparameters: many machine learning algorithms contain in their equations one or more parameters that are not learned with the data, these are known as hyperparameters. For example, linear SVM has the cost hyperparameter C. There is no way of knowing in advance what the exact value of a hyperparameter is that gives rise to the best model, so validation strategies must be used to compare different values.

Prediction: Once the model is created, it is used to predict new observations.

It is throughout this process where the features offered by caret stand out the most, allowing the same syntax to be used to adjust, optimize, evaluate and predict a wide range of models, varying only the name of the algorithm. Although caret allows all this with just a few lines of code, there are many arguments that can be adapted, each with multiple possibilities. In order to better expose each of the options, instead of directly creating a final model, examples are shown from least to most complex.


## Model training


All models built into the caret package are trained with the train () function. The arguments of this function include:

`formula`: the formula of the model to be created.

`x, y`: Instead of a formula, you can pass the values of the predictors and the response variable separately.

`method`: the name of the algorithm to be used (listing) [http://topepo.github.io/caret/available-models.html.

`metric`: the metrics used to assess the predictive power of the model. By default, accuracy is used for classification problems and RMSE for regression Annex 5.

`trControl`: additional specifications on how to carry out model training.

`...`: arguments specific to the algorithm used.


First, we fit a model based on a linear support vector machine that predicts passenger survival based on all available predictors after preprocessing. Except for the algorithm name, all other arguments to the `train` function and svmLinear's own are left by default.



# Models

For this part I chose to build four models. Three simple ones: logistic regression, na√Øve bayes and decision trees. And one a bit more complex: random forest.

Null values where already dropped in a previous cell. Also, categorical variables have already been converted to R factors. Apart from this no more explicit preprocessing was done in this notebook to keep it simple and easy to follow.

To compare the models we first divide the dataset in a training set with 70% of instances and a test set with the rest of the instances. And this taking into account that the distribution of the target has to be the same in both sets.

The test set mimics data in the real world, it will only be used at the end of the project to get a more robust measure of the models on unseen data.

The training set will be used to evaluate the models via 10 fold cross-validation. For simplicity, we'll leave the hypeparameter selection of the models by default, this means that some random combinations will be chosen and the models will be trained via cross-validation for each combination, keeping the hiperparameter combination that gave the best result.


# Bibliography

In addition to the links provided throughout the notebook, the following resources where also consulted to better understand the topic:

- "Heart disease and heart attacks". Khan Academy. Link: [https://www.khanacademy.org/science/health-and-medicine/healthcare-misc/v/heart-disease-and-heart-attacks](https://www.khanacademy.org/science/health-and-medicine/healthcare-misc/v/heart-disease-and-heart-attacks)

- "Cellular respiration introduction". Khan Academy. Link: [https://www.khanacademy.org/science/biology/cellular-respiration-and-fermentation/intro-to-cellular-respiration/v/introduction-to-cellular-respiration](https://www.khanacademy.org/science/biology/cellular-respiration-and-fermentation/intro-to-cellular-respiration/v/introduction-to-cellular-respiration)

- "Enfermedades cardiovasculares". Organizaci√≥n Mundial de la Salud. Link: [https://www.who.int/es/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)](https://www.who.int/es/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds))

- "Heart Disease". Mayo Clinic. Link: [https://www.mayoclinic.org/diseases-conditions/heart-disease/symptoms-causes/syc-20353118](https://www.mayoclinic.org/diseases-conditions/heart-disease/symptoms-causes/syc-20353118)

- "Coronary artery disease". Mayo Clinic. Link: [https://www.mayoclinic.org/diseases-conditions/coronary-artery-disease/symptoms-causes/syc-20350613](https://www.mayoclinic.org/diseases-conditions/coronary-artery-disease/symptoms-causes/syc-20350613)

- "Heart Attack (Myocardial Infarction)". Cleveland Clinic. Link: [https://my.clevelandclinic.org/health/diseases/16818-heart-attack-myocardial-infarction](https://my.clevelandclinic.org/health/diseases/16818-heart-attack-myocardial-infarction)

- "Coronary Artery Disease". Cleveland Clinic. Link: [https://my.clevelandclinic.org/health/diseases/16898-coronary-artery-disease](https://my.clevelandclinic.org/health/diseases/16898-coronary-artery-disease)

- Institute of Medicine (US) Committee on Social Security Cardiovascular Disability Criteria. Cardiovascular Disability: Updating the Social Security Listings. Washington (DC): National Academies Press (US); 2010. 7, Ischemic Heart Disease. Available from: https://www.ncbi.nlm.nih.gov/books/NBK209964/

Hope this was helpful, feel free to give an upvote if you like it. Also, if I made any mistake or if you have any question feel free to write it in the comments üôÇ

